# PageParser

## 网络爬虫工作流程：

```
页面下载器 -> 页面解析器 -> 数据存储

```

`页面下载器`: 主要涉及防爬攻破，方法各异，爬虫的难点也在此

`页面解析器`: 一般页面在一段时间内是固定的，每个人下载页面后都需要解析出页面内容，属于重复工作

`数据存储`: 不管是存储到什么文件或数据库，主要看业务需求

此项目就是将这项工作抽离出来，让网络爬虫程序重点关注于：网页下载，而不是重复的网页解析

## 项目说明

此项目可以和python 的requests 和scrapy 配合使用

当然如果要和其他编程语言使用，可以使用flask等网络框架再次对此项目进行封装，提供网络接口即可

发起人：mouday

发起时间：2018-10-13

需要更多的人一起来维护
